# Ollama Configuration
# Local models by available RAM (assuming no GPU):
# - 16GB: ministral-3:8b or phi3:7b
# - 8GB: llama3.2:3b, phi3:mini, or qwen2.5:3b
# - 4GB: llama3.2:1.5b or qwen2.5:1.5b
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL_ID=qwen2.5:1.5b
